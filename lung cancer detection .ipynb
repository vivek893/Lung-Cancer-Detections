{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23af2c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in c:\\users\\haque computer\\appdata\\roaming\\python\\python311\\site-packages (4.37.2)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\haque computer\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in c:\\users\\haque computer\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\haque computer\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\haque computer\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\haque computer\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.12.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.6.3)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\haque computer\\appdata\\roaming\\python\\python311\\site-packages (from requests->transformers) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\haque computer\\appdata\\roaming\\python\\python311\\site-packages (from requests->transformers) (2023.7.22)\n"
     ]
    }
   ],
   "source": [
    "# Install necessary libraries\n",
    "!pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccab7402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: urllib3[secure] in c:\\users\\haque computer\\appdata\\roaming\\python\\python311\\site-packages (2.2.0)\n",
      "Requirement already satisfied: cryptography in c:\\users\\haque computer\\appdata\\roaming\\python\\python311\\site-packages (42.0.2)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\programdata\\anaconda3\\lib\\site-packages (from cryptography) (1.15.1)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography) (2.21)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: urllib3 2.2.0 does not provide the extra 'secure'\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade urllib3[secure] cryptography\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fba65d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting urllib3==1.25.10\n",
      "  Using cached urllib3-1.25.10-py2.py3-none-any.whl (127 kB)\n",
      "Installing collected packages: urllib3\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.2.0\n",
      "    Uninstalling urllib3-2.2.0:\n",
      "      Successfully uninstalled urllib3-2.2.0\n",
      "Successfully installed urllib3-1.25.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "conda-repo-cli 1.0.41 requires requests_mock, which is not installed.\n",
      "google-auth 2.23.1 requires urllib3>=2.0.5, but you have urllib3 1.25.10 which is incompatible.\n",
      "selenium 4.12.0 requires urllib3[socks]<3,>=1.26, but you have urllib3 1.25.10 which is incompatible.\n",
      "anaconda-client 1.11.3 requires urllib3>=1.26.4, but you have urllib3 1.25.10 which is incompatible.\n",
      "conda-repo-cli 1.0.41 requires clyent==1.2.1, but you have clyent 1.2.2 which is incompatible.\n",
      "conda-repo-cli 1.0.41 requires nbformat==5.4.0, but you have nbformat 5.7.0 which is incompatible.\n",
      "conda-repo-cli 1.0.41 requires requests==2.28.1, but you have requests 2.31.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install urllib3==1.25.10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "036c5349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in c:\\users\\haque computer\\appdata\\roaming\\python\\python311\\site-packages (2.1.2)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (4.6.3)\n",
      "Requirement already satisfied: sympy in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\haque computer\\appdata\\roaming\\python\\python311\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\haque computer\\appdata\\roaming\\python\\python311\\site-packages (from torch) (2023.12.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy->torch) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57568c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20f5e368",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Patient ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Smoking History</th>\n",
       "      <th>Family History</th>\n",
       "      <th>Diagnosis Date</th>\n",
       "      <th>Cancer Stage</th>\n",
       "      <th>Histology Type</th>\n",
       "      <th>Tumor Size</th>\n",
       "      <th>Metastasis</th>\n",
       "      <th>...</th>\n",
       "      <th>Symptom 1</th>\n",
       "      <th>Symptom 2</th>\n",
       "      <th>Symptom 3</th>\n",
       "      <th>Response to Treatment</th>\n",
       "      <th>DFS (months)</th>\n",
       "      <th>OS (months)</th>\n",
       "      <th>Adverse Events</th>\n",
       "      <th>HRQoL Assessment</th>\n",
       "      <th>Follow-up Appointments</th>\n",
       "      <th>Patient-reported Outcomes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P001</td>\n",
       "      <td>70</td>\n",
       "      <td>Male</td>\n",
       "      <td>Smoker</td>\n",
       "      <td>No</td>\n",
       "      <td>20/04/2019</td>\n",
       "      <td>Stage II</td>\n",
       "      <td>Adenocarcinoma</td>\n",
       "      <td>3.93</td>\n",
       "      <td>Brain</td>\n",
       "      <td>...</td>\n",
       "      <td>Loss of appetite</td>\n",
       "      <td>Headaches</td>\n",
       "      <td>Fatigue</td>\n",
       "      <td>Complete Response</td>\n",
       "      <td>60</td>\n",
       "      <td>34</td>\n",
       "      <td>Diarrhea, Neutropenia, Fatigue, Vomiting, Muco...</td>\n",
       "      <td>Improved</td>\n",
       "      <td>['2024-04-07', '2025-12-29', '2025-07-05', '20...</td>\n",
       "      <td>Maintaining a positive outlook despite challen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P002</td>\n",
       "      <td>36</td>\n",
       "      <td>Female</td>\n",
       "      <td>Ex-Smoker</td>\n",
       "      <td>No</td>\n",
       "      <td>07/08/2020</td>\n",
       "      <td>Stage I</td>\n",
       "      <td>Small Cell</td>\n",
       "      <td>7.68</td>\n",
       "      <td>Lung</td>\n",
       "      <td>...</td>\n",
       "      <td>Loss of appetite</td>\n",
       "      <td>Persistent infections</td>\n",
       "      <td>Shortness of breath</td>\n",
       "      <td>Partial Response</td>\n",
       "      <td>18</td>\n",
       "      <td>48</td>\n",
       "      <td>Mucositis, Diarrhea, Fatigue, Hypothyroidism</td>\n",
       "      <td>Stable</td>\n",
       "      <td>['2028-06-03', '2028-06-07', '2028-04-04', '20...</td>\n",
       "      <td>Having challenges with daily activities due to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P003</td>\n",
       "      <td>72</td>\n",
       "      <td>Female</td>\n",
       "      <td>Non-Smoker</td>\n",
       "      <td>Yes</td>\n",
       "      <td>29/12/2022</td>\n",
       "      <td>Stage II</td>\n",
       "      <td>Small Cell</td>\n",
       "      <td>6.26</td>\n",
       "      <td>Brain</td>\n",
       "      <td>...</td>\n",
       "      <td>Difficulty swallowing</td>\n",
       "      <td>Unexplained weight loss</td>\n",
       "      <td>Neurological symptoms (e.g., dizziness, weakness)</td>\n",
       "      <td>Progression</td>\n",
       "      <td>53</td>\n",
       "      <td>53</td>\n",
       "      <td>Neutropenia, Fatigue</td>\n",
       "      <td>Improved</td>\n",
       "      <td>['2024-02-18', '2026-11-08', '2024-09-07', '20...</td>\n",
       "      <td>Feeling generally unwell and fatigued.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P004</td>\n",
       "      <td>77</td>\n",
       "      <td>Female</td>\n",
       "      <td>Non-Smoker</td>\n",
       "      <td>No</td>\n",
       "      <td>21/05/2023</td>\n",
       "      <td>Stage III</td>\n",
       "      <td>Adenocarcinoma</td>\n",
       "      <td>5.39</td>\n",
       "      <td>Brain</td>\n",
       "      <td>...</td>\n",
       "      <td>Headaches</td>\n",
       "      <td>Joint pain</td>\n",
       "      <td>Fatigue</td>\n",
       "      <td>Complete Response</td>\n",
       "      <td>55</td>\n",
       "      <td>70</td>\n",
       "      <td>Pneumonitis</td>\n",
       "      <td>Stable</td>\n",
       "      <td>['2028-03-04', '2025-12-16', '2024-12-16', '20...</td>\n",
       "      <td>Experiencing side effects from treatment.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P005</td>\n",
       "      <td>37</td>\n",
       "      <td>Female</td>\n",
       "      <td>Ex-Smoker</td>\n",
       "      <td>Yes</td>\n",
       "      <td>01/08/2020</td>\n",
       "      <td>Stage IV</td>\n",
       "      <td>Adenocarcinoma</td>\n",
       "      <td>2.51</td>\n",
       "      <td>Liver</td>\n",
       "      <td>...</td>\n",
       "      <td>Difficulty swallowing</td>\n",
       "      <td>Neurological symptoms (e.g., dizziness, weakness)</td>\n",
       "      <td>Persistent cough</td>\n",
       "      <td>Complete Response</td>\n",
       "      <td>35</td>\n",
       "      <td>43</td>\n",
       "      <td>Nausea, Anemia, Vomiting</td>\n",
       "      <td>Improved</td>\n",
       "      <td>['2024-05-11', '2025-10-30', '2024-03-03', '20...</td>\n",
       "      <td>Struggling with the impact of the diagnosis on...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Patient ID  Age  Gender Smoking History Family History Diagnosis Date  \\\n",
       "0       P001   70    Male          Smoker             No     20/04/2019   \n",
       "1       P002   36  Female       Ex-Smoker             No     07/08/2020   \n",
       "2       P003   72  Female      Non-Smoker            Yes     29/12/2022   \n",
       "3       P004   77  Female      Non-Smoker             No     21/05/2023   \n",
       "4       P005   37  Female       Ex-Smoker            Yes     01/08/2020   \n",
       "\n",
       "  Cancer Stage  Histology Type  Tumor Size Metastasis  ...  \\\n",
       "0     Stage II  Adenocarcinoma        3.93      Brain  ...   \n",
       "1      Stage I      Small Cell        7.68       Lung  ...   \n",
       "2     Stage II      Small Cell        6.26      Brain  ...   \n",
       "3    Stage III  Adenocarcinoma        5.39      Brain  ...   \n",
       "4     Stage IV  Adenocarcinoma        2.51      Liver  ...   \n",
       "\n",
       "               Symptom 1                                          Symptom 2  \\\n",
       "0       Loss of appetite                                          Headaches   \n",
       "1       Loss of appetite                              Persistent infections   \n",
       "2  Difficulty swallowing                            Unexplained weight loss   \n",
       "3              Headaches                                         Joint pain   \n",
       "4  Difficulty swallowing  Neurological symptoms (e.g., dizziness, weakness)   \n",
       "\n",
       "                                           Symptom 3 Response to Treatment  \\\n",
       "0                                            Fatigue     Complete Response   \n",
       "1                                Shortness of breath      Partial Response   \n",
       "2  Neurological symptoms (e.g., dizziness, weakness)           Progression   \n",
       "3                                            Fatigue     Complete Response   \n",
       "4                                   Persistent cough     Complete Response   \n",
       "\n",
       "  DFS (months) OS (months)                                     Adverse Events  \\\n",
       "0           60          34  Diarrhea, Neutropenia, Fatigue, Vomiting, Muco...   \n",
       "1           18          48       Mucositis, Diarrhea, Fatigue, Hypothyroidism   \n",
       "2           53          53                               Neutropenia, Fatigue   \n",
       "3           55          70                                        Pneumonitis   \n",
       "4           35          43                           Nausea, Anemia, Vomiting   \n",
       "\n",
       "  HRQoL Assessment                             Follow-up Appointments  \\\n",
       "0         Improved  ['2024-04-07', '2025-12-29', '2025-07-05', '20...   \n",
       "1           Stable  ['2028-06-03', '2028-06-07', '2028-04-04', '20...   \n",
       "2         Improved  ['2024-02-18', '2026-11-08', '2024-09-07', '20...   \n",
       "3           Stable  ['2028-03-04', '2025-12-16', '2024-12-16', '20...   \n",
       "4         Improved  ['2024-05-11', '2025-10-30', '2024-03-03', '20...   \n",
       "\n",
       "                           Patient-reported Outcomes  \n",
       "0  Maintaining a positive outlook despite challen...  \n",
       "1  Having challenges with daily activities due to...  \n",
       "2             Feeling generally unwell and fatigued.  \n",
       "3          Experiencing side effects from treatment.  \n",
       "4  Struggling with the impact of the diagnosis on...  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer_data=pd.read_csv('lung_cancer_dataset.csv')\n",
    "cancer_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac80e133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: urllib3 in c:\\users\\haque computer\\appdata\\roaming\\python\\python311\\site-packages (1.25.10)\n"
     ]
    }
   ],
   "source": [
    "!pip install urllib3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a450980a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Patient ID                       0\n",
       "Age                              0\n",
       "Gender                           0\n",
       "Smoking History                  0\n",
       "Family History                   0\n",
       "Diagnosis Date                   0\n",
       "Cancer Stage                     0\n",
       "Histology Type                   0\n",
       "Tumor Size                       0\n",
       "Metastasis                       0\n",
       "Treatment Type                   0\n",
       "Geneset                          0\n",
       "Genetic Mutations                0\n",
       "Blood Markers                    0\n",
       "IHC Results                      0\n",
       "Radiation Therapy Details        0\n",
       "Chemotherapy Regimen             0\n",
       "Clinical Trial Participation     0\n",
       "Comorbidities                   29\n",
       "ECOG Performance Status          0\n",
       "Symptom 1                        0\n",
       "Symptom 2                        0\n",
       "Symptom 3                        0\n",
       "Response to Treatment            0\n",
       "DFS (months)                     0\n",
       "OS (months)                      0\n",
       "Adverse Events                  14\n",
       "HRQoL Assessment                 0\n",
       "Follow-up Appointments           0\n",
       "Patient-reported Outcomes        0\n",
       "Status                           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e17ab5f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 30)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86b02e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 200 entries, 0 to 199\n",
      "Data columns (total 30 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   Patient ID                    200 non-null    object \n",
      " 1   Age                           200 non-null    int64  \n",
      " 2   Gender                        200 non-null    object \n",
      " 3   Smoking History               200 non-null    object \n",
      " 4   Family History                200 non-null    object \n",
      " 5   Diagnosis Date                200 non-null    object \n",
      " 6   Cancer Stage                  200 non-null    object \n",
      " 7   Histology Type                200 non-null    object \n",
      " 8   Tumor Size                    200 non-null    float64\n",
      " 9   Metastasis                    200 non-null    object \n",
      " 10  Treatment Type                200 non-null    object \n",
      " 11  Geneset                       200 non-null    object \n",
      " 12  Genetic Mutations             200 non-null    object \n",
      " 13  Blood Markers                 200 non-null    object \n",
      " 14  IHC Results                   200 non-null    object \n",
      " 15  Radiation Therapy Details     200 non-null    object \n",
      " 16  Chemotherapy Regimen          200 non-null    object \n",
      " 17  Clinical Trial Participation  200 non-null    object \n",
      " 18  Comorbidities                 171 non-null    object \n",
      " 19  ECOG Performance Status       200 non-null    int64  \n",
      " 20  Symptom 1                     200 non-null    object \n",
      " 21  Symptom 2                     200 non-null    object \n",
      " 22  Symptom 3                     200 non-null    object \n",
      " 23  Response to Treatment         200 non-null    object \n",
      " 24  DFS (months)                  200 non-null    int64  \n",
      " 25  OS (months)                   200 non-null    int64  \n",
      " 26  Adverse Events                186 non-null    object \n",
      " 27  HRQoL Assessment              200 non-null    object \n",
      " 28  Follow-up Appointments        200 non-null    object \n",
      " 29  Patient-reported Outcomes     200 non-null    object \n",
      "dtypes: float64(1), int64(4), object(25)\n",
      "memory usage: 47.0+ KB\n"
     ]
    }
   ],
   "source": [
    "cancer_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20057f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_data=cancer_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "672f4472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(158, 30)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0d13dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in c:\\users\\haque computer\\appdata\\roaming\\python\\python311\\site-packages (4.37.2)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\haque computer\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in c:\\users\\haque computer\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\haque computer\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\haque computer\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\haque computer\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.12.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.6.3)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\haque computer\\appdata\\roaming\\python\\python311\\site-packages (from requests->transformers) (1.25.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\haque computer\\appdata\\roaming\\python\\python311\\site-packages (from requests->transformers) (2023.7.22)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23fd8cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a user input (or 'quit' to stop):  XYZ is a 54-year-old African American male, who presents to the ED with a complaint of a two-week history of hoarseness that is not getting better. When questioned further, he admits that his usual morning \"smoker's cough\" is getting worse and that he has \"given serious thought lately to quitting smoking in the near  future.\"  PMH:  Hypothyroidism 27 years  • Bilateral osteoarthritis of knees 8 years  • Depression 2 years  Iron deficiency anemia of unknown cause, 8 months  Seasonal allergic rhinitis since adolescence  Several episodes of mildly bloody sputum in past year that resolved in a few days; patient  did not seek medical help  No prior history of trauma or surgeries  • Had influenza vaccine last year  Last tetanus booster, 6 years ago.  Family history is negative for lung cancer; however, there is a positive family history for other types of cancer-paternal uncle with colorectal cancer, one niece with non-Hodgkin's lymphoma, and one niece with malignant melanoma Patient is positive for worsening cough and recent onset of shortness of breath with moderate exertion (e.g., climbing one flight of stairs)  • Patient denies all of the following: fever and chills; fatigue; weakness; poor appetite; unintentional eight loss; difficulty or pain with swallowing; pain in chest, abdomen, shoulders, or back; recent onset of vision problems; headache; nausea or vomiting; dizziness; significant swelling; bowel or bladder problems  • Wheeze auscultated in right upper lobe on inspiration  • Percussion reveals area of resonance to dullness over right upper lobe Heart  • Apical pulse normally located at 5th intercostal space at mid-clavicular line Regular rate and rhythm  Normal S1 and S2  (-) murmurs, rubs, S3 and S4  Chest X-Rays  Anteroposteriorandlateralviewsshowa3.5-cmmasslocatedcentrallyinrightupperlobe displacing the right bronchus, roughly corresponding to the area of dullness and wheezing heard by auscultation  Left lung is clear  • No signs of pleural effusion were noted  CT Scans  Chest: Reveals the lesion seen on x-rays plus mediastinal widening and moderately enlarged right-sided hilar nodes; left-sided hilar nodes and all mediastinal nodes appear normal\n",
      "Enter a user input (or 'quit' to stop): quit\n",
      "['XYZ', 'is', 'a', '54-year-old', 'African', 'American', 'male', ',', 'who', 'presents', 'to', 'the', 'ED', 'with', 'a', 'complaint', 'of', 'a', 'two-week', 'history', 'of', 'hoarseness', 'that', 'is', 'not', 'getting', 'better', '.', 'When', 'questioned', 'further', ',', 'he', 'admits', 'that', 'his', 'usual', 'morning', '``', 'smoker', \"'s\", 'cough', \"''\", 'is', 'getting', 'worse', 'and', 'that', 'he', 'has', '``', 'given', 'serious', 'thought', 'lately', 'to', 'quitting', 'smoking', 'in', 'the', 'near', 'future', '.', \"''\", 'PMH', ':', 'Hypothyroidism', '27', 'years', '•', 'Bilateral', 'osteoarthritis', 'of', 'knees', '8', 'years', '•', 'Depression', '2', 'years', 'Iron', 'deficiency', 'anemia', 'of', 'unknown', 'cause', ',', '8', 'months', 'Seasonal', 'allergic', 'rhinitis', 'since', 'adolescence', 'Several', 'episodes', 'of', 'mildly', 'bloody', 'sputum', 'in', 'past', 'year', 'that', 'resolved', 'in', 'a', 'few', 'days', ';', 'patient', 'did', 'not', 'seek', 'medical', 'help', 'No', 'prior', 'history', 'of', 'trauma', 'or', 'surgeries', '•', 'Had', 'influenza', 'vaccine', 'last', 'year', 'Last', 'tetanus', 'booster', ',', '6', 'years', 'ago', '.', 'Family', 'history', 'is', 'negative', 'for', 'lung', 'cancer', ';', 'however', ',', 'there', 'is', 'a', 'positive', 'family', 'history', 'for', 'other', 'types', 'of', 'cancer-paternal', 'uncle', 'with', 'colorectal', 'cancer', ',', 'one', 'niece', 'with', 'non-Hodgkin', \"'s\", 'lymphoma', ',', 'and', 'one', 'niece', 'with', 'malignant', 'melanoma', 'Patient', 'is', 'positive', 'for', 'worsening', 'cough', 'and', 'recent', 'onset', 'of', 'shortness', 'of', 'breath', 'with', 'moderate', 'exertion', '(', 'e.g.', ',', 'climbing', 'one', 'flight', 'of', 'stairs', ')', '•', 'Patient', 'denies', 'all', 'of', 'the', 'following', ':', 'fever', 'and', 'chills', ';', 'fatigue', ';', 'weakness', ';', 'poor', 'appetite', ';', 'unintentional', 'eight', 'loss', ';', 'difficulty', 'or', 'pain', 'with', 'swallowing', ';', 'pain', 'in', 'chest', ',', 'abdomen', ',', 'shoulders', ',', 'or', 'back', ';', 'recent', 'onset', 'of', 'vision', 'problems', ';', 'headache', ';', 'nausea', 'or', 'vomiting', ';', 'dizziness', ';', 'significant', 'swelling', ';', 'bowel', 'or', 'bladder', 'problems', '•', 'Wheeze', 'auscultated', 'in', 'right', 'upper', 'lobe', 'on', 'inspiration', '•', 'Percussion', 'reveals', 'area', 'of', 'resonance', 'to', 'dullness', 'over', 'right', 'upper', 'lobe', 'Heart', '•', 'Apical', 'pulse', 'normally', 'located', 'at', '5th', 'intercostal', 'space', 'at', 'mid-clavicular', 'line', 'Regular', 'rate', 'and', 'rhythm', 'Normal', 'S1', 'and', 'S2', '(', '-', ')', 'murmurs', ',', 'rubs', ',', 'S3', 'and', 'S4', 'Chest', 'X-Rays', 'Anteroposteriorandlateralviewsshowa3.5-cmmasslocatedcentrallyinrightupperlobe', 'displacing', 'the', 'right', 'bronchus', ',', 'roughly', 'corresponding', 'to', 'the', 'area', 'of', 'dullness', 'and', 'wheezing', 'heard', 'by', 'auscultation', 'Left', 'lung', 'is', 'clear', '•', 'No', 'signs', 'of', 'pleural', 'effusion', 'were', 'noted', 'CT', 'Scans', 'Chest', ':', 'Reveals', 'the', 'lesion', 'seen', 'on', 'x-rays', 'plus', 'mediastinal', 'widening', 'and', 'moderately', 'enlarged', 'right-sided', 'hilar', 'nodes', ';', 'left-sided', 'hilar', 'nodes', 'and', 'all', 'mediastinal', 'nodes', 'appear', 'normal']\n"
     ]
    }
   ],
   "source": [
    "# Sample Medical Transcript\n",
    "user_inputs = []\n",
    "while True:\n",
    "    user_input = input(\"Enter a user input (or 'quit' to stop): \")\n",
    "    if user_input.lower() == \"quit\":\n",
    "        break\n",
    "    user_inputs.append(user_input)\n",
    "for i in user_inputs:\n",
    "    # Tokenize using NLTK\n",
    "    tokens = word_tokenize(i)\n",
    "    tokenizer = word_tokenize\n",
    "# Output the tokens\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e7162df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "lemmatizer=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0c1e1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "def get_simple_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3918e833",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'!',\n",
       "  '\"',\n",
       "  '#',\n",
       "  '$',\n",
       "  '%',\n",
       "  '&',\n",
       "  \"'\",\n",
       "  '(',\n",
       "  ')',\n",
       "  '*',\n",
       "  '+',\n",
       "  ',',\n",
       "  '-',\n",
       "  '.',\n",
       "  '/',\n",
       "  ':',\n",
       "  ';',\n",
       "  '<',\n",
       "  '=',\n",
       "  '>',\n",
       "  '?',\n",
       "  '@',\n",
       "  '[',\n",
       "  '\\\\',\n",
       "  ']',\n",
       "  '^',\n",
       "  '_',\n",
       "  '`',\n",
       "  'a',\n",
       "  'about',\n",
       "  'above',\n",
       "  'after',\n",
       "  'again',\n",
       "  'against',\n",
       "  'ain',\n",
       "  'all',\n",
       "  'am',\n",
       "  'an',\n",
       "  'and',\n",
       "  'any',\n",
       "  'are',\n",
       "  'aren',\n",
       "  \"aren't\",\n",
       "  'as',\n",
       "  'at',\n",
       "  'be',\n",
       "  'because',\n",
       "  'been',\n",
       "  'before',\n",
       "  'being',\n",
       "  'below',\n",
       "  'between',\n",
       "  'both',\n",
       "  'but',\n",
       "  'by',\n",
       "  'can',\n",
       "  'couldn',\n",
       "  \"couldn't\",\n",
       "  'd',\n",
       "  'did',\n",
       "  'didn',\n",
       "  \"didn't\",\n",
       "  'do',\n",
       "  'does',\n",
       "  'doesn',\n",
       "  \"doesn't\",\n",
       "  'doing',\n",
       "  'don',\n",
       "  \"don't\",\n",
       "  'down',\n",
       "  'during',\n",
       "  'each',\n",
       "  'few',\n",
       "  'for',\n",
       "  'from',\n",
       "  'further',\n",
       "  'had',\n",
       "  'hadn',\n",
       "  \"hadn't\",\n",
       "  'has',\n",
       "  'hasn',\n",
       "  \"hasn't\",\n",
       "  'have',\n",
       "  'haven',\n",
       "  \"haven't\",\n",
       "  'having',\n",
       "  'he',\n",
       "  'her',\n",
       "  'here',\n",
       "  'hers',\n",
       "  'herself',\n",
       "  'him',\n",
       "  'himself',\n",
       "  'his',\n",
       "  'how',\n",
       "  'i',\n",
       "  'if',\n",
       "  'in',\n",
       "  'into',\n",
       "  'is',\n",
       "  'isn',\n",
       "  \"isn't\",\n",
       "  'it',\n",
       "  \"it's\",\n",
       "  'its',\n",
       "  'itself',\n",
       "  'just',\n",
       "  'll',\n",
       "  'm',\n",
       "  'ma',\n",
       "  'me',\n",
       "  'mightn',\n",
       "  \"mightn't\",\n",
       "  'more',\n",
       "  'most',\n",
       "  'mustn',\n",
       "  \"mustn't\",\n",
       "  'my',\n",
       "  'myself',\n",
       "  'needn',\n",
       "  \"needn't\",\n",
       "  'no',\n",
       "  'nor',\n",
       "  'not',\n",
       "  'now',\n",
       "  'o',\n",
       "  'of',\n",
       "  'off',\n",
       "  'on',\n",
       "  'once',\n",
       "  'only',\n",
       "  'or',\n",
       "  'other',\n",
       "  'our',\n",
       "  'ours',\n",
       "  'ourselves',\n",
       "  'out',\n",
       "  'over',\n",
       "  'own',\n",
       "  're',\n",
       "  's',\n",
       "  'same',\n",
       "  'shan',\n",
       "  \"shan't\",\n",
       "  'she',\n",
       "  \"she's\",\n",
       "  'should',\n",
       "  \"should've\",\n",
       "  'shouldn',\n",
       "  \"shouldn't\",\n",
       "  'so',\n",
       "  'some',\n",
       "  'such',\n",
       "  't',\n",
       "  'than',\n",
       "  'that',\n",
       "  \"that'll\",\n",
       "  'the',\n",
       "  'their',\n",
       "  'theirs',\n",
       "  'them',\n",
       "  'themselves',\n",
       "  'then',\n",
       "  'there',\n",
       "  'these',\n",
       "  'they',\n",
       "  'this',\n",
       "  'those',\n",
       "  'through',\n",
       "  'to',\n",
       "  'too',\n",
       "  'under',\n",
       "  'until',\n",
       "  'up',\n",
       "  've',\n",
       "  'very',\n",
       "  'was',\n",
       "  'wasn',\n",
       "  \"wasn't\",\n",
       "  'we',\n",
       "  'were',\n",
       "  'weren',\n",
       "  \"weren't\",\n",
       "  'what',\n",
       "  'when',\n",
       "  'where',\n",
       "  'which',\n",
       "  'while',\n",
       "  'who',\n",
       "  'whom',\n",
       "  'why',\n",
       "  'will',\n",
       "  'with',\n",
       "  'won',\n",
       "  \"won't\",\n",
       "  'wouldn',\n",
       "  \"wouldn't\",\n",
       "  'y',\n",
       "  'you',\n",
       "  \"you'd\",\n",
       "  \"you'll\",\n",
       "  \"you're\",\n",
       "  \"you've\",\n",
       "  'your',\n",
       "  'yours',\n",
       "  'yourself',\n",
       "  'yourselves',\n",
       "  '{',\n",
       "  '|',\n",
       "  '}',\n",
       "  '~'},\n",
       " '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "stops=set(stopwords.words('english'))\n",
    "punctuations=list(string.punctuation)\n",
    "stops.update(punctuations)\n",
    "stops,string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1748213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose stemming or lemmatization based on your preference\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def normalize_text(text, stem=True):\n",
    "    \"\"\"\n",
    "    Normalizes text by tokenizing, removing stop words, and applying stemming/lemmatization.\n",
    "    \"\"\"\n",
    "    tokens = tokenizer(text.lower())\n",
    "    filtered_tokens = [token for token in tokens if token not in stops]\n",
    "    if stem:\n",
    "        normalized_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "    else:\n",
    "        normalized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "    return \" \".join(normalized_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b007235",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_reviews(words):\n",
    "    output_reviews=[]\n",
    "    for w in words:\n",
    "        if w.lower() not in stops:\n",
    "            pos=pos_tag([w])\n",
    "            #lemmatize the word\n",
    "            clean_review=lemmatizer.lemmatize(w,pos=get_simple_pos(pos[0][1]))\n",
    "            output_reviews.append(clean_review.lower())\n",
    "    return output_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cdf80300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xyz',\n",
       " '54-year-old',\n",
       " 'african',\n",
       " 'american',\n",
       " 'male',\n",
       " 'present',\n",
       " 'ed',\n",
       " 'complaint',\n",
       " 'two-week',\n",
       " 'history',\n",
       " 'hoarseness',\n",
       " 'get',\n",
       " 'well',\n",
       " 'question',\n",
       " 'admits',\n",
       " 'usual',\n",
       " 'morning',\n",
       " '``',\n",
       " 'smoker',\n",
       " \"'s\",\n",
       " 'cough',\n",
       " \"''\",\n",
       " 'get',\n",
       " 'bad',\n",
       " '``',\n",
       " 'give',\n",
       " 'serious',\n",
       " 'thought',\n",
       " 'lately',\n",
       " 'quit',\n",
       " 'smoking',\n",
       " 'near',\n",
       " 'future',\n",
       " \"''\",\n",
       " 'pmh',\n",
       " 'hypothyroidism',\n",
       " '27',\n",
       " 'year',\n",
       " '•',\n",
       " 'bilateral',\n",
       " 'osteoarthritis',\n",
       " 'knee',\n",
       " '8',\n",
       " 'year',\n",
       " '•',\n",
       " 'depression',\n",
       " '2',\n",
       " 'year',\n",
       " 'iron',\n",
       " 'deficiency',\n",
       " 'anemia',\n",
       " 'unknown',\n",
       " 'cause',\n",
       " '8',\n",
       " 'month',\n",
       " 'seasonal',\n",
       " 'allergic',\n",
       " 'rhinitis',\n",
       " 'since',\n",
       " 'adolescence',\n",
       " 'several',\n",
       " 'episode',\n",
       " 'mildly',\n",
       " 'bloody',\n",
       " 'sputum',\n",
       " 'past',\n",
       " 'year',\n",
       " 'resolve',\n",
       " 'day',\n",
       " 'patient',\n",
       " 'seek',\n",
       " 'medical',\n",
       " 'help',\n",
       " 'prior',\n",
       " 'history',\n",
       " 'trauma',\n",
       " 'surgery',\n",
       " '•',\n",
       " 'influenza',\n",
       " 'vaccine',\n",
       " 'last',\n",
       " 'year',\n",
       " 'last',\n",
       " 'tetanus',\n",
       " 'booster',\n",
       " '6',\n",
       " 'year',\n",
       " 'ago',\n",
       " 'family',\n",
       " 'history',\n",
       " 'negative',\n",
       " 'lung',\n",
       " 'cancer',\n",
       " 'however',\n",
       " 'positive',\n",
       " 'family',\n",
       " 'history',\n",
       " 'type',\n",
       " 'cancer-paternal',\n",
       " 'uncle',\n",
       " 'colorectal',\n",
       " 'cancer',\n",
       " 'one',\n",
       " 'niece',\n",
       " 'non-hodgkin',\n",
       " \"'s\",\n",
       " 'lymphoma',\n",
       " 'one',\n",
       " 'niece',\n",
       " 'malignant',\n",
       " 'melanoma',\n",
       " 'patient',\n",
       " 'positive',\n",
       " 'worsen',\n",
       " 'cough',\n",
       " 'recent',\n",
       " 'onset',\n",
       " 'shortness',\n",
       " 'breath',\n",
       " 'moderate',\n",
       " 'exertion',\n",
       " 'e.g.',\n",
       " 'climb',\n",
       " 'one',\n",
       " 'flight',\n",
       " 'stair',\n",
       " '•',\n",
       " 'patient',\n",
       " 'denies',\n",
       " 'follow',\n",
       " 'fever',\n",
       " 'chill',\n",
       " 'fatigue',\n",
       " 'weakness',\n",
       " 'poor',\n",
       " 'appetite',\n",
       " 'unintentional',\n",
       " 'eight',\n",
       " 'loss',\n",
       " 'difficulty',\n",
       " 'pain',\n",
       " 'swallow',\n",
       " 'pain',\n",
       " 'chest',\n",
       " 'abdomen',\n",
       " 'shoulder',\n",
       " 'back',\n",
       " 'recent',\n",
       " 'onset',\n",
       " 'vision',\n",
       " 'problem',\n",
       " 'headache',\n",
       " 'nausea',\n",
       " 'vomit',\n",
       " 'dizziness',\n",
       " 'significant',\n",
       " 'swell',\n",
       " 'bowel',\n",
       " 'bladder',\n",
       " 'problem',\n",
       " '•',\n",
       " 'wheeze',\n",
       " 'auscultate',\n",
       " 'right',\n",
       " 'upper',\n",
       " 'lobe',\n",
       " 'inspiration',\n",
       " '•',\n",
       " 'percussion',\n",
       " 'reveals',\n",
       " 'area',\n",
       " 'resonance',\n",
       " 'dullness',\n",
       " 'right',\n",
       " 'upper',\n",
       " 'lobe',\n",
       " 'heart',\n",
       " '•',\n",
       " 'apical',\n",
       " 'pulse',\n",
       " 'normally',\n",
       " 'locate',\n",
       " '5th',\n",
       " 'intercostal',\n",
       " 'space',\n",
       " 'mid-clavicular',\n",
       " 'line',\n",
       " 'regular',\n",
       " 'rate',\n",
       " 'rhythm',\n",
       " 'normal',\n",
       " 's1',\n",
       " 's2',\n",
       " 'murmur',\n",
       " 'rub',\n",
       " 's3',\n",
       " 's4',\n",
       " 'chest',\n",
       " 'x-rays',\n",
       " 'anteroposteriorandlateralviewsshowa3.5-cmmasslocatedcentrallyinrightupperlobe',\n",
       " 'displace',\n",
       " 'right',\n",
       " 'bronchus',\n",
       " 'roughly',\n",
       " 'correspond',\n",
       " 'area',\n",
       " 'dullness',\n",
       " 'wheeze',\n",
       " 'heard',\n",
       " 'auscultation',\n",
       " 'left',\n",
       " 'lung',\n",
       " 'clear',\n",
       " '•',\n",
       " 'sign',\n",
       " 'pleural',\n",
       " 'effusion',\n",
       " 'note',\n",
       " 'ct',\n",
       " 'scans',\n",
       " 'chest',\n",
       " 'reveals',\n",
       " 'lesion',\n",
       " 'see',\n",
       " 'x-ray',\n",
       " 'plus',\n",
       " 'mediastinal',\n",
       " 'widen',\n",
       " 'moderately',\n",
       " 'enlarge',\n",
       " 'right-sided',\n",
       " 'hilar',\n",
       " 'node',\n",
       " 'left-sided',\n",
       " 'hilar',\n",
       " 'node',\n",
       " 'mediastinal',\n",
       " 'node',\n",
       " 'appear',\n",
       " 'normal']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medical_transcript=clean_reviews(tokens)\n",
    "medical_transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "99346033",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Difficulty swallowing Unexplained weight loss Neurological symptoms (e.g., dizziness, weakness)'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer_data[\"Symptom1\"]=cancer_data[\"Symptom 1\"]+\" \"+cancer_data[\"Symptom 2\"]+\" \"+cancer_data[\"Symptom 3\"]\n",
    "cancer_data[\"Symptom1\"][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ae250d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine relevant text columns into a single 'symptoms' column\n",
    "numeric_features = ['Age', 'Tumor Size',\"ECOG Performance Status\"]\n",
    "textual_feature = 'medical_transcript'\n",
    "text_columns = ['Geneset', 'Genetic Mutations', 'Gender', 'Smoking History',\"Symptom 1\",\"Symptom 2\",\"Symptom 3\",'Cancer Stage', 'Histology Type','Metastasis','Treatment Type',\"Comorbidities\",\"Clinical Trial Participation\",\"IHC Results\",\"Blood Markers\",\"Family History\"]\n",
    "cancer_data['symptoms'] = cancer_data[text_columns].apply(lambda x: ' '.join(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0845cd32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      KRAS, HER2, PIK3CA KRAS G12V, HER2 HER2 Amplif...\n",
       "2      CDKN2A, ATM, BCL2 CDKN2A CDKN2A Mutation, ATM ...\n",
       "3      RET, KRAS, EGFR RET RET Fusion, KRAS G12V, EGF...\n",
       "5      FGFR2, DDR2, SOX2 FGFR2 FGFR2 Amplification, D...\n",
       "6      ERBB2, BRAF, PIK3CA ERBB2 HER2 Amplification, ...\n",
       "                             ...                        \n",
       "195    DDR2, FGFR2, NOTCH1 DDR2 DDR2 Mutation, FGFR2 ...\n",
       "196    CDKN2A, NOTCH1, FGFR1 CDKN2A CDKN2A Mutation, ...\n",
       "197    RET, KRAS, MET RET RET Fusion, KRAS G12V, MET ...\n",
       "198    BRAF, ERBB2, PIK3CA BRAF D594G, ERBB2 HER2 Amp...\n",
       "199    DDR2, FGFR1, NOTCH1 DDR2 DDR2 Mutation, FGFR1 ...\n",
       "Name: symptoms, Length: 158, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer_data['symptoms']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2dc49ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 158 entries, 0 to 199\n",
      "Data columns (total 32 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   Patient ID                    158 non-null    object \n",
      " 1   Age                           158 non-null    int64  \n",
      " 2   Gender                        158 non-null    object \n",
      " 3   Smoking History               158 non-null    object \n",
      " 4   Family History                158 non-null    object \n",
      " 5   Diagnosis Date                158 non-null    object \n",
      " 6   Cancer Stage                  158 non-null    object \n",
      " 7   Histology Type                158 non-null    object \n",
      " 8   Tumor Size                    158 non-null    float64\n",
      " 9   Metastasis                    158 non-null    object \n",
      " 10  Treatment Type                158 non-null    object \n",
      " 11  Geneset                       158 non-null    object \n",
      " 12  Genetic Mutations             158 non-null    object \n",
      " 13  Blood Markers                 158 non-null    object \n",
      " 14  IHC Results                   158 non-null    object \n",
      " 15  Radiation Therapy Details     158 non-null    object \n",
      " 16  Chemotherapy Regimen          158 non-null    object \n",
      " 17  Clinical Trial Participation  158 non-null    object \n",
      " 18  Comorbidities                 158 non-null    object \n",
      " 19  ECOG Performance Status       158 non-null    int64  \n",
      " 20  Symptom 1                     158 non-null    object \n",
      " 21  Symptom 2                     158 non-null    object \n",
      " 22  Symptom 3                     158 non-null    object \n",
      " 23  Response to Treatment         158 non-null    object \n",
      " 24  DFS (months)                  158 non-null    int64  \n",
      " 25  OS (months)                   158 non-null    int64  \n",
      " 26  Adverse Events                158 non-null    object \n",
      " 27  HRQoL Assessment              158 non-null    object \n",
      " 28  Follow-up Appointments        158 non-null    object \n",
      " 29  Patient-reported Outcomes     158 non-null    object \n",
      " 30  Symptom1                      158 non-null    object \n",
      " 31  symptoms                      158 non-null    object \n",
      "dtypes: float64(1), int64(4), object(27)\n",
      "memory usage: 44.8+ KB\n"
     ]
    }
   ],
   "source": [
    "cancer_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "23f714f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:           Age  Gender  Smoking History  Family History  Tumor Size  \\\n",
      "171 -0.425161       0                1               1   -1.565853   \n",
      "71  -0.808719       0                0               0   -0.934142   \n",
      "159  0.853366       0                0               1    0.907459   \n",
      "105 -1.320130       0                1               0   -0.757476   \n",
      "196 -0.744793       0                1               0    0.902106   \n",
      "\n",
      "     Treatment Type  Blood Markers  IHC Results  Clinical Trial Participation  \\\n",
      "171               0            102           38                             0   \n",
      "71                0             60           98                             0   \n",
      "159               1             77           52                             1   \n",
      "105               2             65           49                             0   \n",
      "196               1             36           19                             0   \n",
      "\n",
      "     Comorbidities  ECOG Performance Status  Symptom 1  Symptom 2  Symptom 3  \\\n",
      "171             81                 1.457896         10          8         14   \n",
      "71              20                -1.093422         11          0          5   \n",
      "159             78                 0.182237         17         12          5   \n",
      "105             40                 1.457896          2          1         12   \n",
      "196             93                -1.093422          0          6          3   \n",
      "\n",
      "     Patient-reported Outcomes  \n",
      "171                          1  \n",
      "71                           1  \n",
      "159                          3  \n",
      "105                          1  \n",
      "196                          3  \n",
      "y_train:      Geneset  Genetic Mutations  Cancer Stage  Histology Type  Metastasis\n",
      "171       38                 38             0               2           3\n",
      "71        98                 98             3               0           1\n",
      "159       52                 55             1               1           0\n",
      "105       49                 49             1               1           3\n",
      "196       19                 19             3               2           0\n",
      "X_test:           Age  Gender  Smoking History  Family History  Tumor Size  \\\n",
      "161  0.022323       1                1               0    0.575543   \n",
      "55  -0.616940       1                1               0    1.581999   \n",
      "168 -0.105529       1                2               1   -1.480197   \n",
      "198 -0.297308       0                1               1    1.464222   \n",
      "115 -0.936572       1                0               0    1.405334   \n",
      "\n",
      "     Treatment Type  Blood Markers  IHC Results  Clinical Trial Participation  \\\n",
      "161               1             40           -1                             1   \n",
      "55                0             61           -1                             1   \n",
      "168               0             88           -1                             1   \n",
      "198               0             15           -1                             1   \n",
      "115               0             -1           73                             1   \n",
      "\n",
      "     Comorbidities  ECOG Performance Status  Symptom 1  Symptom 2  Symptom 3  \\\n",
      "161             -1                 0.182237         13          3          7   \n",
      "55              -1                -1.093422          2          4         16   \n",
      "168             58                -1.093422          7          2          1   \n",
      "198             -1                -1.093422          8          5          9   \n",
      "115             -1                 1.457896          5          9          1   \n",
      "\n",
      "     Patient-reported Outcomes  \n",
      "161                          6  \n",
      "55                           7  \n",
      "168                          4  \n",
      "198                          6  \n",
      "115                          2  \n",
      "y_test:      Geneset  Genetic Mutations  Cancer Stage  Histology Type  Metastasis\n",
      "161       -1                 -1             2               2           3\n",
      "55        -1                 -1             0               0           1\n",
      "168       -1                 -1             3               1           3\n",
      "198       -1                 -1             2               0           3\n",
      "115       73                 72             3               0           2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "# Assuming your target variables include 'Geneset', 'Genetic Mutations', 'Cancer Stage', 'Histology Type', and 'Metastasis'\n",
    "target_columns = ['Geneset', 'Genetic Mutations', 'Cancer Stage', 'Histology Type', 'Metastasis']\n",
    "\n",
    "# Features (excluding the target columns)\n",
    "# Assuming your target variable is 'lung_cancer_status'\n",
    "# Features (excluding the target columns)\n",
    "X = cancer_data.drop(target_columns + ['Patient ID', 'Diagnosis Date', 'Chemotherapy Regimen', 'HRQoL Assessment', \n",
    "                                       'Symptom1', 'symptoms', 'Follow-up Appointments', 'Adverse Events', \n",
    "                                       'Radiation Therapy Details', 'Response to Treatment', 'DFS (months)', 'OS (months)'], \n",
    "                    axis=1) # Features\n",
    "y = cancer_data[target_columns]  # Target variable\n",
    "\n",
    "# Split the data into training and testing sets (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Data Preprocessing for Gene and Transcript Data\n",
    "# (Assuming you have numerical features in the gene and transcript data)\n",
    "numeric_features = ['Age', 'Tumor Size', \"ECOG Performance Status\"]\n",
    "\n",
    "# Standardize numerical features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train[numeric_features] = scaler.fit_transform(X_train[numeric_features])\n",
    "X_test[numeric_features] = scaler.transform(X_test[numeric_features])\n",
    "\n",
    "# Data Preprocessing for Medical Transcript Data\n",
    "# (Assuming you have categorical features in the medical transcript data)\n",
    "text_columns = ['Gender', 'Smoking History', \"Symptom 1\", \"Symptom 2\", \"Symptom 3\",\n",
    "                'Treatment Type', \"Comorbidities\", \"Clinical Trial Participation\", \n",
    "                \"IHC Results\", \"Blood Markers\", \"Family History\",\"Patient-reported Outcomes\"]\n",
    "\n",
    "\n",
    "# Encode categorical features using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "for feature in text_columns:\n",
    "    # Fit and transform the training set\n",
    "    X_train[feature] = label_encoder.fit_transform(X_train[feature])\n",
    "\n",
    "    # Handle unknown labels in the test set\n",
    "    X_test[feature] = X_test[feature].map(\n",
    "        lambda s: label_encoder.transform([s])[0] if s in label_encoder.classes_ else -1\n",
    "    )\n",
    "for feature in target_columns:\n",
    "    # Fit and transform the training set\n",
    "    y_train[feature] = label_encoder.fit_transform(y_train[feature])\n",
    "\n",
    "    # Handle unknown labels in the test set\n",
    "    y_test[feature] = y_test[feature].map(\n",
    "        lambda s: label_encoder.transform([s])[0] if s in label_encoder.classes_ else -1\n",
    "    )\n",
    "\n",
    "# Display the preprocessed data\n",
    "# Display the preprocessed data\n",
    "print(\"X_train:\", X_train.head())\n",
    "print(\"y_train:\", y_train.head())\n",
    "print(\"X_test:\", X_test.head())\n",
    "print(\"y_test:\", y_test.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "73360fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenSSL 1.1.1w  11 Sep 2023\n"
     ]
    }
   ],
   "source": [
    "import ssl\n",
    "print(ssl.OPENSSL_VERSION)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "22a56031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tensorflow in c:\\users\\haque computer\\appdata\\roaming\\python\\python311\\site-packages (2.15.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.15.0 in c:\\users\\haque computer\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\haque computer\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\haque computer\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\haque computer\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\haque computer\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\haque computer\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.7.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\haque computer\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in c:\\users\\haque computer\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.24.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\haque computer\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (23.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\haque computer\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (4.23.4)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (67.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\haque computer\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (4.6.3)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\haque computer\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\haque computer\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.58.0)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in c:\\users\\haque computer\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in c:\\users\\haque computer\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in c:\\users\\haque computer\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.15.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\haque computer\\appdata\\roaming\\python\\python311\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.23.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in c:\\users\\haque computer\\appdata\\roaming\\python\\python311\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\haque computer\\appdata\\roaming\\python\\python311\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\haque computer\\appdata\\roaming\\python\\python311\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.7.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\haque computer\\appdata\\roaming\\python\\python311\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\haque computer\\appdata\\roaming\\python\\python311\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (4.9)\n",
      "Collecting urllib3>=2.0.5 (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached urllib3-2.2.0-py3-none-any.whl (120 kB)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\haque computer\\appdata\\roaming\\python\\python311\\site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\haque computer\\appdata\\roaming\\python\\python311\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\haque computer\\appdata\\roaming\\python\\python311\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.2.2)\n",
      "Installing collected packages: urllib3\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.25.10\n",
      "    Uninstalling urllib3-1.25.10:\n",
      "      Successfully uninstalled urllib3-1.25.10\n",
      "Successfully installed urllib3-2.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "conda-repo-cli 1.0.41 requires requests_mock, which is not installed.\n",
      "botocore 1.27.59 requires urllib3<1.27,>=1.25.4, but you have urllib3 2.2.0 which is incompatible.\n",
      "conda-repo-cli 1.0.41 requires clyent==1.2.1, but you have clyent 1.2.2 which is incompatible.\n",
      "conda-repo-cli 1.0.41 requires nbformat==5.4.0, but you have nbformat 5.7.0 which is incompatible.\n",
      "conda-repo-cli 1.0.41 requires requests==2.28.1, but you have requests 2.31.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8c5b3be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: keras in c:\\users\\haque computer\\appdata\\roaming\\python\\python311\\site-packages (2.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3af327bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age                             float64\n",
      "Gender                            int32\n",
      "Smoking History                   int32\n",
      "Family History                    int32\n",
      "Tumor Size                      float64\n",
      "Treatment Type                    int32\n",
      "Blood Markers                     int32\n",
      "IHC Results                       int32\n",
      "Clinical Trial Participation      int32\n",
      "Comorbidities                     int32\n",
      "ECOG Performance Status         float64\n",
      "Symptom 1                         int32\n",
      "Symptom 2                         int32\n",
      "Symptom 3                         int32\n",
      "Patient-reported Outcomes         int32\n",
      "dtype: object\n",
      "y_train shape Geneset              int32\n",
      "Genetic Mutations    int32\n",
      "Cancer Stage         int32\n",
      "Histology Type       int32\n",
      "Metastasis           int32\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(X_train.dtypes)\n",
    "print(\"y_train shape\", y_train.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cd4fe0c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (126, 15)\n",
      "Shape of y_train: (126, 5)\n",
      "numpy array of y_train : [[ 38  38   0   2   3]\n",
      " [ 98  98   3   0   1]\n",
      " [ 52  55   1   1   0]\n",
      " [ 49  49   1   1   3]\n",
      " [ 19  19   3   2   0]\n",
      " [ 39  39   1   2   3]\n",
      " [ 50  54   0   1   3]\n",
      " [ 34  34   1   2   3]\n",
      " [ 66  66   1   2   3]\n",
      " [ 37  37   2   2   1]\n",
      " [ 11  11   1   1   0]\n",
      " [113 113   3   1   3]\n",
      " [ 83  83   0   2   0]\n",
      " [119 119   2   1   0]\n",
      " [  0   0   3   0   2]\n",
      " [120 120   1   1   1]\n",
      " [ 63  63   0   1   3]\n",
      " [ 55  52   0   0   2]\n",
      " [100 100   3   0   1]\n",
      " [ 13  13   3   0   3]\n",
      " [112 112   0   1   2]\n",
      " [ 91  91   3   1   0]\n",
      " [121 121   2   1   3]\n",
      " [116 116   1   1   3]\n",
      " [ 23  23   1   2   0]\n",
      " [ 31  31   3   0   3]\n",
      " [105 105   3   1   3]\n",
      " [ 17  17   1   1   3]\n",
      " [106 106   2   2   2]\n",
      " [ 12  12   1   0   3]\n",
      " [ 99  99   3   0   3]\n",
      " [ 89  89   3   1   0]\n",
      " [104 104   3   1   0]\n",
      " [ 29  29   2   2   0]\n",
      " [ 81  81   3   1   0]\n",
      " [ 51  50   1   0   0]\n",
      " [ 47  47   1   1   2]\n",
      " [  4   4   2   1   0]\n",
      " [ 60  60   2   0   2]\n",
      " [ 16  16   2   2   3]\n",
      " [110 110   0   2   2]\n",
      " [ 82  82   1   1   1]\n",
      " [ 74  73   0   0   2]\n",
      " [ 59  59   2   0   2]\n",
      " [ 79  79   1   2   0]\n",
      " [108 108   1   2   1]\n",
      " [ 86  86   1   2   1]\n",
      " [ 48  48   1   1   0]\n",
      " [ 18  18   1   1   1]\n",
      " [ 25  25   0   2   3]\n",
      " [ 75  74   0   0   3]\n",
      " [  2   2   2   1   2]\n",
      " [ 97  97   1   0   0]\n",
      " [  3   3   1   1   3]\n",
      " [ 35  35   3   2   3]\n",
      " [  8   8   0   1   2]\n",
      " [ 24  24   1   2   0]\n",
      " [  1   1   0   0   1]\n",
      " [  6   6   2   1   3]\n",
      " [ 22  22   1   1   2]\n",
      " [123 123   1   2   1]\n",
      " [ 30  30   0   2   1]\n",
      " [ 56  53   0   0   0]\n",
      " [118 118   3   2   0]\n",
      " [ 87  87   2   1   1]\n",
      " [ 67  67   2   2   2]\n",
      " [ 93  93   1   0   3]\n",
      " [ 40  40   3   2   1]\n",
      " [115 115   0   2   2]\n",
      " [ 45  45   0   0   0]\n",
      " [ 41  41   0   2   3]\n",
      " [ 70  70   2   2   3]\n",
      " [ 54  56   3   1   1]\n",
      " [ 76  75   0   0   3]\n",
      " [ 44  44   0   0   2]\n",
      " [  9   9   3   1   3]\n",
      " [ 14  14   3   0   3]\n",
      " [ 85  85   0   1   2]\n",
      " [109 109   1   2   2]\n",
      " [ 71  71   0   0   0]\n",
      " [ 58  58   0   0   0]\n",
      " [107 107   0   2   0]\n",
      " [ 80  80   2   2   0]\n",
      " [117 117   1   2   2]\n",
      " [103 103   3   0   1]\n",
      " [ 62  62   3   1   3]\n",
      " [ 36  36   1   2   2]\n",
      " [ 92  92   0   1   0]\n",
      " [ 10  10   0   1   2]\n",
      " [ 28  28   2   2   3]\n",
      " [ 73  72   0   0   0]\n",
      " [ 27  27   1   2   3]\n",
      " [122 122   0   1   1]\n",
      " [ 84  84   3   2   1]\n",
      " [ 94  94   2   0   0]\n",
      " [ 43  43   3   0   0]\n",
      " [ 53  51   1   0   1]\n",
      " [ 99  99   2   0   0]\n",
      " [101 101   2   0   2]\n",
      " [ 88  88   1   1   0]\n",
      " [ 95  95   0   0   1]\n",
      " [ 68  68   0   2   3]\n",
      " [ 20  20   0   1   0]\n",
      " [111 111   0   1   3]\n",
      " [ 46  46   3   1   2]\n",
      " [ 32  32   1   2   2]\n",
      " [ 42  42   2   0   1]\n",
      " [102 102   0   0   2]\n",
      " [ 78  76   0   0   0]\n",
      " [ 64  64   0   1   3]\n",
      " [ 15  15   1   1   0]\n",
      " [ 72  77   2   2   0]\n",
      " [ 65  65   2   1   2]\n",
      " [ 61  61   3   1   2]\n",
      " [114 114   3   1   3]\n",
      " [ 21  21   1   1   3]\n",
      " [  7   7   3   1   1]\n",
      " [ 69  69   3   2   3]\n",
      " [ 57  57   0   1   1]\n",
      " [ 23  23   3   2   0]\n",
      " [ 96  96   2   0   2]\n",
      " [ 90  90   1   1   3]\n",
      " [ 26  26   2   2   0]\n",
      " [ 33  33   1   2   1]\n",
      " [ 77  78   2   2   3]\n",
      " [  5   5   0   1   1]]\n",
      "Unique values in y_train: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123]\n"
     ]
    }
   ],
   "source": [
    "# Check the shape of X_train\n",
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "\n",
    "# Check the shape of y_train\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "y_train = np.array(y_train)\n",
    "print(\"numpy array of y_train :\",y_train)\n",
    "# Print unique values in y_train\n",
    "print(\"Unique values in y_train:\", np.unique(y_train))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1f5b3f3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = len(np.unique(y_train))\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0d9fcc6f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'lib' has no attribute 'X509_V_FLAG_NOTIFY_POLICY'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dense, Dropout\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\__init__.py:54\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m autograph\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bitwise\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compat\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\_api\\v2\\compat\\__init__.py:8\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.compat namespace\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m v1\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m v2\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m forward_compatibility_horizon \u001b[38;5;66;03m# line: 125\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\_api\\v2\\compat\\v1\\__init__.py:32\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m autograph\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bitwise\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compat\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\_api\\v2\\compat\\v1\\compat\\__init__.py:9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m v1\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m v2\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m forward_compatibility_horizon \u001b[38;5;66;03m# line: 125\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m forward_compatible \u001b[38;5;66;03m# line: 65\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\_api\\v2\\compat\\v1\\compat\\v2\\__init__.py:28\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LazyLoader \u001b[38;5;28;01mas\u001b[39;00m _LazyLoader\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KerasLazyLoader \u001b[38;5;28;01mas\u001b[39;00m _KerasLazyLoader\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __internal__\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __operators__\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m audio\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\_api\\v2\\compat\\v2\\__init__.py:34\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m autograph\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bitwise\n\u001b[1;32m---> 34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compat\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\_api\\v2\\compat\\v2\\compat\\__init__.py:9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m v1\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m v2\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m forward_compatibility_horizon \u001b[38;5;66;03m# line: 125\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m forward_compatible \u001b[38;5;66;03m# line: 65\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\_api\\v2\\compat\\v2\\compat\\v2\\__init__.py:341\u001b[0m\n\u001b[0;32m    339\u001b[0m _current_module \u001b[38;5;241m=\u001b[39m _sys\u001b[38;5;241m.\u001b[39mmodules[\u001b[38;5;18m__name__\u001b[39m]\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 341\u001b[0m   \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorboard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msummary\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_tf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m summary\n\u001b[0;32m    342\u001b[0m   _current_module\u001b[38;5;241m.\u001b[39m__path__ \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    343\u001b[0m       [_module_util\u001b[38;5;241m.\u001b[39mget_parent_dir(summary)] \u001b[38;5;241m+\u001b[39m _current_module\u001b[38;5;241m.\u001b[39m__path__)\n\u001b[0;32m    344\u001b[0m   \u001b[38;5;28msetattr\u001b[39m(_current_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummary\u001b[39m\u001b[38;5;124m\"\u001b[39m, summary)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorboard\\summary\\__init__.py:22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# If the V1 summary API is accessible, load and re-export it here.\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 22\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorboard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msummary\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m v1  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorboard\\summary\\v1.py:23\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorboard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplugins\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m summary \u001b[38;5;28;01mas\u001b[39;00m _audio_summary\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorboard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplugins\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcustom_scalar\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m summary \u001b[38;5;28;01mas\u001b[39;00m _custom_scalar_summary\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorboard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplugins\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhistogram\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m summary \u001b[38;5;28;01mas\u001b[39;00m _histogram_summary\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorboard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplugins\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m summary \u001b[38;5;28;01mas\u001b[39;00m _image_summary\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorboard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplugins\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpr_curve\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m summary \u001b[38;5;28;01mas\u001b[39;00m _pr_curve_summary\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorboard\\plugins\\histogram\\summary.py:35\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorboard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplugins\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhistogram\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m metadata\n\u001b[1;32m---> 35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorboard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplugins\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhistogram\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m summary_v2\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Export V3 versions.\u001b[39;00m\n\u001b[0;32m     39\u001b[0m histogram \u001b[38;5;241m=\u001b[39m summary_v2\u001b[38;5;241m.\u001b[39mhistogram\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorboard\\plugins\\histogram\\summary_v2.py:35\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorboard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplugins\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhistogram\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m metadata\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorboard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lazy_tensor_creator\n\u001b[1;32m---> 35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorboard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor_util\n\u001b[0;32m     38\u001b[0m DEFAULT_BUCKET_COUNT \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m30\u001b[39m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhistogram_pb\u001b[39m(tag, data, buckets\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, description\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorboard\\util\\tensor_util.py:20\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorboard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mproto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor_pb2\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorboard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorflow_stub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dtypes, compat, tensor_shape\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mExtractBitsFromFloat16\u001b[39m(x):\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray(x, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat16)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint16)\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorboard\\compat\\tensorflow_stub\\__init__.py:22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorboard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mproto\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmeta_graph_pb2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorboard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mproto\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msummary_pb2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m as_dtype  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DType  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m string  \u001b[38;5;66;03m# noqa\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:19\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Library of dtypes (Tensor element types).\"\"\"\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorboard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mproto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m types_pb2\n\u001b[0;32m     22\u001b[0m _np_bfloat16 \u001b[38;5;241m=\u001b[39m pywrap_tensorflow\u001b[38;5;241m.\u001b[39mTF_bfloat16_type()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorboard\\compat\\tensorflow_stub\\pywrap_tensorflow.py:22\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstruct\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m errors\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gfile\n\u001b[0;32m     25\u001b[0m TFE_DEVICE_PLACEMENT_WARN \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     26\u001b[0m TFE_DEVICE_PLACEMENT_SILENT_FOR_INT32 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorboard\\compat\\tensorflow_stub\\io\\__init__.py:17\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gfile\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorboard\\compat\\tensorflow_stub\\io\\gfile.py:33\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mboto3\u001b[39;00m\n\u001b[0;32m     35\u001b[0m     S3_ENABLED \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\boto3\\__init__.py:17\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mboto3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _warn_deprecated_python\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mboto3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msession\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Session\n\u001b[0;32m     19\u001b[0m __author__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAmazon Web Services\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     20\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1.24.28\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\boto3\\session.py:17\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcopy\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msession\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Config\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataNotFoundError, UnknownServiceError\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\botocore\\session.py:26\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msocket\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfigloader\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcredentials\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\botocore\\client.py:15\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\"). You\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# ANY KIND, either express or implied. See the License for the specific\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# language governing permissions and limitations under the License.\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m waiter, xform_name\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01margs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ClientArgsCreator\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauth\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AUTH_TYPE_MAPS\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\botocore\\waiter.py:18\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjmespath\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocstring\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WaiterDocstring\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_service_module_name\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m xform_name\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\botocore\\docs\\__init__.py:15\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\"). You\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# ANY KIND, either express or implied. See the License for the specific\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# language governing permissions and limitations under the License.\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mservice\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ServiceDocumenter\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_docs\u001b[39m(root_dir, session):\n\u001b[0;32m     19\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Generates the reference documentation for botocore\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \n\u001b[0;32m     21\u001b[0m \u001b[38;5;124;03m    This will go through every available AWS service and output ReSTructured\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;124;03m        root_dir/reference/services/service-name.rst\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\botocore\\docs\\service.py:14\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\"). You\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# ANY KIND, either express or implied. See the License for the specific\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# language governing permissions and limitations under the License.\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbcdoc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrestdoc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DocumentStructure\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ClientDocumenter, ClientExceptionsDocumenter\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpaginator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PaginatorDocumenter\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwaiter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WaiterDocumenter\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\botocore\\docs\\client.py:14\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\"). You\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# ANY KIND, either express or implied. See the License for the specific\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# language governing permissions and limitations under the License.\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OrderedDict\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexample\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ResponseExampleDocumenter\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmethod\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     16\u001b[0m     document_custom_method,\n\u001b[0;32m     17\u001b[0m     document_model_driven_method,\n\u001b[0;32m     18\u001b[0m     get_instance_public_methods,\n\u001b[0;32m     19\u001b[0m )\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparams\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ResponseParamsDocumenter\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\botocore\\docs\\example.py:13\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\"). You\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# ANY KIND, either express or implied. See the License for the specific\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# language governing permissions and limitations under the License.\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mshape\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ShapeDocumenter\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m py_default\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBaseExampleDocumenter\u001b[39;00m(ShapeDocumenter):\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\botocore\\docs\\shape.py:19\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\"). You\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# inherited from a Documenter class with the appropriate methods\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# and attributes.\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_json_value_header\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mShapeDocumenter\u001b[39;00m:\n\u001b[0;32m     23\u001b[0m     EVENT_NAME \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\botocore\\utils.py:34\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mawsrequest\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhttpsession\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# IP Regexes retained for backwards compatibility\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HEX_PAT  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\botocore\\httpsession.py:41\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mssl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OP_NO_TICKET, PROTOCOL_TLS_CLIENT\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;66;03m# Always import the original SSLContext, even if it has been patched\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01murllib3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontrib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyopenssl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m orig_util_SSLContext \u001b[38;5;28;01mas\u001b[39;00m SSLContext\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01murllib3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mssl_\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SSLContext\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\urllib3\\contrib\\pyopenssl.py:43\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mModule for using pyOpenSSL as a TLS backend. This module was relevant before\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mthe standard library ``ssl`` module supported SNI, but now that we've dropped\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03m.. _idna: https://github.com/kjd/idna\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m annotations\n\u001b[1;32m---> 43\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mOpenSSL\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mSSL\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[import]\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcryptography\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m x509\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\OpenSSL\\__init__.py:8\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright (C) AB Strakt\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# See LICENSE for details.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03mpyOpenSSL - A simple wrapper around the OpenSSL library\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mOpenSSL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SSL, crypto\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mOpenSSL\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     10\u001b[0m     __author__,\n\u001b[0;32m     11\u001b[0m     __copyright__,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m     __version__,\n\u001b[0;32m     18\u001b[0m )\n\u001b[0;32m     21\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSSL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcrypto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__version__\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     32\u001b[0m ]\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\OpenSSL\\SSL.py:19\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mweakref\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WeakValueDictionary\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mOpenSSL\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_util\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     10\u001b[0m     UNSPECIFIED \u001b[38;5;28;01mas\u001b[39;00m _UNSPECIFIED,\n\u001b[0;32m     11\u001b[0m     exception_from_error_queue \u001b[38;5;28;01mas\u001b[39;00m _exception_from_error_queue,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m     text_to_bytes_and_warn \u001b[38;5;28;01mas\u001b[39;00m _text_to_bytes_and_warn,\n\u001b[0;32m     18\u001b[0m )\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mOpenSSL\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcrypto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     20\u001b[0m     FILETYPE_PEM,\n\u001b[0;32m     21\u001b[0m     PKey,\n\u001b[0;32m     22\u001b[0m     X509,\n\u001b[0;32m     23\u001b[0m     X509Name,\n\u001b[0;32m     24\u001b[0m     X509Store,\n\u001b[0;32m     25\u001b[0m     _PassphraseHelper,\n\u001b[0;32m     26\u001b[0m )\n\u001b[0;32m     28\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENSSL_VERSION_NUMBER\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSSLEAY_VERSION\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    126\u001b[0m ]\n\u001b[0;32m    129\u001b[0m OPENSSL_VERSION_NUMBER \u001b[38;5;241m=\u001b[39m _lib\u001b[38;5;241m.\u001b[39mOPENSSL_VERSION_NUMBER\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\OpenSSL\\crypto.py:1616\u001b[0m\n\u001b[0;32m   1612\u001b[0m         ext\u001b[38;5;241m.\u001b[39m_extension \u001b[38;5;241m=\u001b[39m _ffi\u001b[38;5;241m.\u001b[39mgc(extension, _lib\u001b[38;5;241m.\u001b[39mX509_EXTENSION_free)\n\u001b[0;32m   1613\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m ext\n\u001b[1;32m-> 1616\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mX509StoreFlags\u001b[39;00m:\n\u001b[0;32m   1617\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1618\u001b[0m \u001b[38;5;124;03m    Flags for X509 verification, used to change the behavior of\u001b[39;00m\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;124;03m    :class:`X509Store`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1624\u001b[0m \u001b[38;5;124;03m        https://www.openssl.org/docs/manmaster/man3/X509_VERIFY_PARAM_set_flags.html\u001b[39;00m\n\u001b[0;32m   1625\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   1627\u001b[0m     CRL_CHECK: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m _lib\u001b[38;5;241m.\u001b[39mX509_V_FLAG_CRL_CHECK\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\OpenSSL\\crypto.py:1635\u001b[0m, in \u001b[0;36mX509StoreFlags\u001b[1;34m()\u001b[0m\n\u001b[0;32m   1633\u001b[0m EXPLICIT_POLICY: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m _lib\u001b[38;5;241m.\u001b[39mX509_V_FLAG_EXPLICIT_POLICY\n\u001b[0;32m   1634\u001b[0m INHIBIT_MAP: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m _lib\u001b[38;5;241m.\u001b[39mX509_V_FLAG_INHIBIT_MAP\n\u001b[1;32m-> 1635\u001b[0m NOTIFY_POLICY: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m _lib\u001b[38;5;241m.\u001b[39mX509_V_FLAG_NOTIFY_POLICY\n\u001b[0;32m   1636\u001b[0m CHECK_SS_SIGNATURE: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m _lib\u001b[38;5;241m.\u001b[39mX509_V_FLAG_CHECK_SS_SIGNATURE\n\u001b[0;32m   1637\u001b[0m PARTIAL_CHAIN: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m _lib\u001b[38;5;241m.\u001b[39mX509_V_FLAG_PARTIAL_CHAIN\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'lib' has no attribute 'X509_V_FLAG_NOTIFY_POLICY'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "import numpy as np\n",
    "\n",
    "# Assuming y_train is numeric\n",
    "# Convert y_train to NumPy array\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "# Modify the last layer of your model for regression\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))  # Use 'linear' activation for regression\n",
    "\n",
    "# Compile the model with 'mean_squared_error' loss for regression\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mse'])\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()\n",
    "\n",
    "# Train the DNN model using the training dataset\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d66863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and normalize symptoms\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def normalize_symptoms(text):\n",
    "    tokens = text.lower().split()\n",
    "    filtered_tokens = [token for token in tokens if token not in stops]\n",
    "    normalized_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "    return \" \".join(normalized_tokens)\n",
    "\n",
    "cancer_data['normalized_symptoms'] = cancer_data['symptoms'].apply(normalize_symptoms)\n",
    "print(cancer_data['normalized_symptoms'][0])\n",
    "# Create a TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(cancer_data['normalized_symptoms'])\n",
    "print(tfidf_matrix)\n",
    "tfidf_matrix.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76152d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather user inputs\n",
    "tfidf_features = vectorizer.transform(medical_transcript)\n",
    "tfidf_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d07a5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Load pre-trained BioBERT model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"monologg/biobert_v1.1_pubmed\")\n",
    "model_bert = AutoModel.from_pretrained(\"monologg/biobert_v1.1_pubmed\")\n",
    "\n",
    "# Tokenize user input \n",
    "tokenized_user_input = tokenizer(user_inputs, is_split_into_words=True, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "print(tokenized_user_input)\n",
    "# Get word embeddings\n",
    "with torch.no_grad():\n",
    "    outputs = model_bert(**tokenized_user_input)\n",
    "    word_embeddings = outputs.last_hidden_state.numpy()\n",
    "word_embeddings.shape,word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5960fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5dabae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a model with NER capabilities (e.g., en_core_web_sm for general English)\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "for user_input in user_inputs:\n",
    "    doc = nlp(user_input)\n",
    "    ## Render output by using displacy module\n",
    "    displacy.render(doc, style=\"dep\")\n",
    "    # Extract entities and their labels\n",
    "    entities = [(entity.text, entity.label_) for entity in doc.ents]\n",
    "print(entities)\n",
    "entity_types = [entity[1] for entity in entities]\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "entities_features = encoder.fit_transform(np.array(entity_types).reshape(-1, 1)).toarray()\n",
    "print(entities_features.shape)\n",
    "print(entities_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cae1fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Load a model with NER capabilities (e.g., en_core_web_sm for general English)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# List of user inputs (assuming you have it defined somewhere)\n",
    "user_inputs = [\"Your first user input\", \"Your second user input\"]\n",
    "\n",
    "# Extract entities and their labels for each user input\n",
    "all_entities = []\n",
    "for user_input in medical_transcript:\n",
    "    doc = nlp(user_input)\n",
    "    # Render output using displacy module (dependency visualization)\n",
    "    displacy.render(doc, style=\"dep\")\n",
    "    \n",
    "    # Extract entities and their labels\n",
    "    entities = [(entity.text, entity.label_) for entity in doc.ents]\n",
    "    all_entities.extend(entities)\n",
    "\n",
    "# Print all entities\n",
    "print(\"All entities:\", all_entities)\n",
    "\n",
    "# Extract entity types\n",
    "entity_types = [entity[1] for entity in all_entities]\n",
    "\n",
    "# Reshape entity types before using OneHotEncoder\n",
    "entity_types_reshaped = np.array(entity_types).reshape(-1, 1)\n",
    "\n",
    "# Initialize OneHotEncoder\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# Fit and transform entity types\n",
    "entities_features = encoder.fit_transform(entity_types_reshaped).toarray()\n",
    "\n",
    "# Print the shape and features\n",
    "print(\"Shape of entities features:\", entities_features.shape)\n",
    "print(\"Entities features:\")\n",
    "print(entities_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d636de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from nltk import flatten\n",
    "tokens_df= cancer_data['Geneset'] #drop null values\n",
    "# Get top 30 tokens based upon frequency in whole corpus\n",
    "word_freq_top30 = pd.DataFrame(collections.Counter(flatten(tokens_df.to_list())).most_common(),columns=['words',\"frequency\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5315fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq_top30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8129df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Plot bar graph of words wrt frequency \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set(style=\"whitegrid\")\n",
    "sns.set(rc={'figure.figsize':(20,11)})\n",
    "ax = sns.barplot(x=word_freq_top30['words'][1:], y=word_freq_top30['frequency'])\n",
    "ax.set_xticklabels(ax.get_xticklabels(),rotation=75)\n",
    "plt.xticks(rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d029b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Combine the outputs from DNN and BioBERT\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "from tensorflow.keras.layers import Reshape\n",
    "\n",
    "# Reshape model.output to have the same dimensions as word_embeddings\n",
    "reshaped_dnn_output = Reshape((1, 1))(model.output)\n",
    "print(reshaped_dnn_output.shape)\n",
    "print(word_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc41a107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the outputs from DNN and BioBERT\n",
    "from tensorflow.keras.layers import Concatenate, RepeatVector, Input, Reshape\n",
    "\n",
    "input_shape = (10, 769)  # Adjust the size based on your actual input shapes\n",
    "\n",
    "# Create an Input layer with the specified input shape\n",
    "input_layer = Input(shape=input_shape)\n",
    "\n",
    "# Repeat the vector to match the word_embeddings shape\n",
    "reshaped_dnn_output = RepeatVector(512)(model.output)\n",
    "print(reshaped_dnn_output.shape)  # Output: (None, 512, 1)\n",
    "\n",
    "# Reshape to desired output\n",
    "reshaped_dnn_output = Reshape((512, 1))(reshaped_dnn_output)\n",
    "print(reshaped_dnn_output.shape)  # Output: (None, 512, 1)\n",
    "\n",
    "# Now both reshaped_dnn_output and word_embeddings have compatible shapes\n",
    "combined_features = Concatenate(axis=-1)([reshaped_dnn_output, word_embeddings])\n",
    "print(combined_features.shape)\n",
    "\n",
    "# Add a dense layer for further processing or prediction\n",
    "combined_model = Sequential()\n",
    "combined_model.add(Dense(64, activation='relu', input_shape=(512, 769)))  # Adjust input shape based on the combined_features.shape\n",
    "combined_model.add(Dense(1, activation='linear'))  # Adjust activation based on the task (regression)\n",
    "\n",
    "# Compile the combined model\n",
    "combined_model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mse'])\n",
    "\n",
    "# Display the combined model summary\n",
    "combined_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2231d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469bcfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5774b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(combined_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace6484e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbb5a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c98e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96673e57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Concatenate, Input, Reshape\n",
    "\n",
    "# Create an Input layer with the specified input shape\n",
    "input_layer = Input(shape=(512, 769), name='input_layer')\n",
    "\n",
    "# Design the final model architecture\n",
    "dense_layer_1 = Dense(64, activation='relu')(input_layer)\n",
    "dense_layer_2 = Dense(32, activation='relu')(dense_layer_1)\n",
    "\n",
    "# Output layers for each target variable\n",
    "geneset_output = Dense(1, activation='linear', name='geneset_output')(dense_layer_2)\n",
    "mutation_output = Dense(1, activation='linear', name='mutation_output')(dense_layer_2)\n",
    "histology_output = Dense(1, activation='linear', name='histology_output')(dense_layer_2)\n",
    "cancer_stage_output = Dense(1, activation='linear', name='cancer_stage_output')(dense_layer_2)\n",
    "metastasis_output = Dense(1, activation='linear', name='metastasis_output')(dense_layer_2)\n",
    "\n",
    "# Create the final model\n",
    "final_model = Model(inputs=input_layer, outputs=[geneset_output, mutation_output, histology_output,\n",
    "                                                  cancer_stage_output, metastasis_output])\n",
    "\n",
    "# Compile the final model\n",
    "final_model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mse'])\n",
    "\n",
    "# Display the final model summary\n",
    "final_model.summary()\n",
    "\n",
    "# Train the final model using your data\n",
    "final_model.fit(combined_features.numpy(),\n",
    "                {'geneset_output': cancer_data['Geneset'], \n",
    "                 'mutation_output': cancer_data['Genetic Mutations'], \n",
    "                 'histology_output': cancer_data['Histology Type'],\n",
    "                 'cancer_stage_output': cancer_data['Cancer Stage'],\n",
    "                 'metastasis_output': cancer_data['Metastasis']},\n",
    "                epochs=10, batch_size=32, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502e2e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a separate testing dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def preprocess_data(data):\n",
    "    # Assuming 'data' is a DataFrame\n",
    "    numeric_features = ['Age', 'Tumor Size', 'ECOG Performance Status']\n",
    "    \n",
    "    # Standardize numerical features using StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    data[numeric_features] = scaler.transform(data[numeric_features])\n",
    "    \n",
    "    # Assuming 'text_columns' are the categorical features\n",
    "    text_columns = ['Gender', 'Smoking History', 'Symptom 1', 'Symptom 2', 'Symptom 3',\n",
    "                    'Treatment Type', 'Comorbidities', 'Clinical Trial Participation', \n",
    "                    'IHC Results', 'Blood Markers', 'Family History', 'Patient-reported Outcomes']\n",
    "    \n",
    "    # Encode categorical features using LabelEncoder\n",
    "    label_encoder = LabelEncoder()\n",
    "    for feature in text_columns:\n",
    "        data[feature] = label_encoder.transform(data[feature])\n",
    "    \n",
    "    return data\n",
    "x_data=preprocess_data(medical_transcript)\n",
    "print(x_data)\n",
    "X_test_preprocessed = preprocess_data(X_test)  # Apply the same preprocessing as during training\n",
    "\n",
    "# Evaluate the model on the testing dataset\n",
    "evaluation_results = final_model.evaluate(X_test_preprocessed,\n",
    "                                          {'geneset_output': y_test['Geneset'], \n",
    "                                           'mutation_output': y_test['Genetic Mutations'], \n",
    "                                           'histology_output': y_test['Histology Type'],\n",
    "                                           'cancer_stage_output': y_test['Cancer Stage'],\n",
    "                                           'metastasis_output': y_test['Metastasis']},\n",
    "                                          batch_size=32)\n",
    "\n",
    "# Print the evaluation results\n",
    "print(\"Evaluation Results:\")\n",
    "print(\"Loss:\", evaluation_results[0])\n",
    "print(\"Mean Squared Error (MSE):\", evaluation_results[1])\n",
    "\n",
    "# Assuming 'user_input' is the new data for prediction\n",
    "user_input_preprocessed = preprocess_data(user_input)  # Apply the same preprocessing as during training\n",
    "\n",
    "# Make predictions on the new data\n",
    "predictions = final_model.predict(user_input_preprocessed)\n",
    "\n",
    "# Display the predictions\n",
    "print(\"Predictions:\")\n",
    "print(\"Geneset Output:\", predictions[0][0])\n",
    "print(\"Mutation Output:\", predictions[1][0])\n",
    "print(\"Histology Output:\", predictions[2][0])\n",
    "print(\"Cancer Stage Output:\", predictions[3][0])\n",
    "print(\"Metastasis Output:\", predictions[4][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b14d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "Feature extraction typically occurs during the training phase of a machine learning model. In the context of deep learning models like DNNs, feature extraction is an inherent part of the training process. During training, the model learns to extract relevant features from the input data to make predictions.\n",
    "\n",
    "Here's a more detailed breakdown of the process:\n",
    "\n",
    "1)Data Splitting:\n",
    "\n",
    "Split your dataset into training and testing sets. This is done at the beginning to assess the model's performance on unseen data.\n",
    "Common splits include 80% of the data for training and 20% for testing.\n",
    "2.)Data Preprocessing:\n",
    "\n",
    "Preprocess both the gene and transcript data and the medical transcript data. This involves normalizing, scaling, or encoding features as necessary.\n",
    "3.)DNN Model Training:\n",
    "\n",
    "Design and define the architecture of your DNN model, considering the specific characteristics of your gene and transcript data.\n",
    "Compile the model with an appropriate optimizer, loss function, and evaluation metric.\n",
    "Train the DNN model using the training dataset.\n",
    "The DNN learns to automatically extract relevant features during the training process.\n",
    "4.)BioBERT Fine-tuning:\n",
    "\n",
    "Fine-tune BioBERT on your medical transcript data during the training phase.\n",
    "Extract features from the BioBERT model, either during fine-tuning or later.\n",
    "5.)Integration of DNN and BioBERT:\n",
    "\n",
    "Combine the outputs from the DNN model (gene and transcript data) with the features extracted from BioBERT (medical transcript data).\n",
    "This integration step can happen after training both the DNN and BioBERT.\n",
    "6.)Final Model Training:\n",
    "\n",
    "Design a final model that takes the integrated data as input and predicts geneset name, genetic mutation, and histology type related to lung cancer.\n",
    "Train the final model using the integrated data.\n",
    "7.)Model Evaluation:\n",
    "\n",
    "Evaluate the performance of the final model on the testing dataset.\n",
    "Assess how well the model generalizes to new, unseen data.\n",
    "Feature Extraction during Testing/User Input:\n",
    "\n",
    "When using the model for predictions on new, unseen data (testing or user input), feature extraction is still part of the inference process.\n",
    "The model automatically extracts relevant features from the input data to make predictions.\n",
    "In summary, feature extraction is an intrinsic part of the training and prediction process in machine learning models. During training, the model learns to extract features, and during testing or inference, it applies this knowledge to new data for making predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
